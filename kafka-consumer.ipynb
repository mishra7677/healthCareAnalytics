{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7ed8293b-59fa-467e-a415-3ef3b64749ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./install-liberaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e7120ff-8d9e-4380-8afc-4f02a036e1f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Collecting confluent_kafka\n",
      "  Downloading confluent_kafka-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.0/4.0 MB 11.9 MB/s eta 0:00:00\n",
      "Installing collected packages: confluent_kafka\n",
      "Successfully installed confluent_kafka-2.3.0\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "import os \n",
    "from dotenv import load_env\n",
    "\n",
    "\n",
    "class Bronze:\n",
    "    \"\"\"\n",
    "    - ingest data from kafka\n",
    "    - transform value from binary to string\n",
    "    - transform string value to struct type\n",
    "    - save the stream to bronze table\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.load_env(),\n",
    "        self.BOOTSTRAP_SERVERS= f\"{os.getenv('bootstrap')}\",\n",
    "        self.protocol= f\"{os.getenv('protocol')}\",\n",
    "        self.mechanism= f\"{os.getenv('mechanism')}\",\n",
    "        self.JAAS_MODULE= f\"{os.getenv('JAAS_MODULE')}\",\n",
    "        self.CLUSTER_API_KEY= f\"{os.getenv('api_key')}\",\n",
    "        self.CLUSTER_API_SECRETS= f\"{os.getenv('password')}\"\n",
    "\n",
    "\n",
    "        pass\n",
    "    def getSchema(self):\n",
    "        \"\"\"return the schema for the json\"\"\"\n",
    "        return ( StructType([\n",
    "                        StructField(\"device_id\", IntegerType()),\n",
    "                        StructField(\"sensor_type\", StringType()),\n",
    "                        StructField(\"reading_value\", IntegerType()),\n",
    "                        StructField(\"patient_id\", StringType()),\n",
    "                        StructField(\"timestamp\", TimestampType())\n",
    "                ]))\n",
    "            \n",
    "\n",
    "    def ingestFromKafka(self):\n",
    "        return (\n",
    "            spark.readStream.format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", self.BOOTSTRAP_SERVERS)\n",
    "            .option(\"kafka.security.protocol\", self.protocol)\n",
    "            .option(\"kafka.sasl.mechanism\", self.mechanism)\n",
    "            .option(\"kafka.sasl.jaas.config\",f\"{self.JAAS_MODULE} required username='{self.CLUSTER_API_KEY}' password='{self.CLUSTER_API_SECRETS}';\",\n",
    "            )\n",
    "            .option(\"subscribe\", \"topic_0\")\n",
    "            .load()\n",
    "        )\n",
    "\n",
    "    def getReading(self, kafka_df):\n",
    "        return kafka_df.select(\n",
    "            f.col(\"key\").cast(\"string\"), f.col(\"value\").cast(\"string\")\n",
    "        )\n",
    "\n",
    "    def process(self):\n",
    "        print(\"starting Bronze Stream...\")\n",
    "\n",
    "        rawDF = self.ingestFromKafka()\n",
    "        readingDF = self.getReading(rawDF)\n",
    "\n",
    "        processDF= readingDF.withColumn(\n",
    "            \"json_value\", f.from_json(f.col(\"value\"), self.getSchema())\n",
    "            ).select(\n",
    "                f.col(\"json_value.device_id\").alias(\"device_id\"),\n",
    "                f.col(\"json_value.sensor_type\").alias(\"sensor\"),\n",
    "                f.col(\"json_value.reading_value\").alias(\"reading\"),\n",
    "                f.col(\"json_value.timestamp\").alias(\"timestamp\"),\n",
    "                f.col(\"json_value.patient_id\").alias(\"patient_id\"),\n",
    "            )\n",
    "        sQuery = (\n",
    "            processDF.writeStream.queryName(\"bronze-ingestion\")\n",
    "            .option(\"checkpointLocation\", f\"{self.base_data_dir}/chekpoint/deviceReading_bz\")\n",
    "            .outputMode(\"append\")\n",
    "            .toTable(\"device_reading_bz\")\n",
    "        )\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34fadf85-5047-432c-abed-cbc5ac4dde38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class silver():\n",
    "    ''' - read streaming bronze tabel \n",
    "        -read patient_details json file\n",
    "        -create dataframe from patient_details json\n",
    "        -find the max reading in a window of 10 with an interval of 2 min \n",
    "        -calculate new col `condition` based on the max reading \n",
    "        - sink the data to silver layer \n",
    "        '''\n",
    "    def __init__(self):\n",
    "        self.jPATH=\"dbfs:/FileStore/patient.json\"\n",
    "        self.bzPATH=\"device_reading_bz\"\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    def getPatient(self):\n",
    "       from pyspark.sql import functions as f\n",
    "       return(\n",
    "           spark.read.format(\"json\")\n",
    "                .option(\"multiline\",\"True\")\n",
    "                .load(self.jPATH)\\\n",
    "                .select(\n",
    "                    f.col('key').alias('pid'),\n",
    "                    f.col('value.name'),\n",
    "                    f.col('value.age'),\n",
    "                    f.col('value.gender'),\n",
    "                    f.col('value.demographics.marital_status'),\n",
    "                    f.col('value.demographics.ethencity'),\n",
    "                    f.col('value.demographics.address'),\n",
    "                    f.col('value.demographics.contact_number'),\n",
    "                    f.col('value.insurance_info.insurance_provider'),\n",
    "                    f.col('value.insurance_info.validity'),\n",
    "                    f.col('value.insurance_info.policy_number')\n",
    "                )\n",
    "       )\n",
    "\n",
    "    def readBronze(self):\n",
    "        return(\n",
    "            spark.readStream.table(self.bzPATH)\n",
    "                    )\n",
    "    def getAggregate(self, sensorDF):\n",
    "        print(\"starting silver layer processing .....\")\n",
    "        from pyspark.sql.functions import window, max,substring\n",
    "\n",
    "        patientDF = self.getPatient()  # Assuming getPatient returns a DataFrame\n",
    "        return (\n",
    "            sensorDF.withWatermark(\"timestamp\", \"30 Minutes\")\n",
    "            .groupby(\n",
    "                sensorDF.patient_id, sensorDF.device_id, sensorDF.sensor,\n",
    "                window(\"timestamp\", \"10 Minutes\", \"2 Minutes\").alias(\"window\")\n",
    "            )\n",
    "            .agg(max(sensorDF.reading).alias(\"maxReading\"))\n",
    "            .join(\n",
    "                patientDF,\n",
    "                substring(patientDF.pid, 4, 12) == sensorDF.patient_id,\n",
    "                \"inner\"\n",
    "            )\n",
    "            .select(\n",
    "                sensorDF.patient_id, patientDF.name, patientDF.age, sensorDF.sensor,\n",
    "                sensorDF.device_id, \"window.start\", \"window.end\", \"maxReading\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "\n",
    "    def process(self):\n",
    "        sensorDF = self.readBronze()\n",
    "        sQuery = self.getAggregate(sensorDF).writeStream.queryName(\"bronze-ingestion\")\\\n",
    "                                            .option(\"checkpointLocation\", f\"{self.base_data_dir}/chekpoint/deviceReading_bz\")\\\n",
    "                                            .outputMode(\"append\")\\\n",
    "                                            .toTable(\"CareSl\")       \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5c09128-959d-4e72-b5b1-a98c3eb8802e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting Bronze Stream...\n"
     ]
    }
   ],
   "source": [
    "bz=Bronze()\n",
    "bz.process()\n",
    "sl=silver()\n",
    "sl.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78ccedd9-3fdf-4859-a91a-fed5310e5a36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1307964110880498,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "kafka-consumer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
